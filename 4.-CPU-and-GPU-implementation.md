- [4.1 General Information](#41-general-information)
- [4.2 Main Steps of the Code](#42-main-steps-of-the-code)
- [4.3 GPU Implementation](#43-gpu-implementation)
- [4.4 Double Precision](#44-double-precision)
- [4.5 OpenMP for multi-core executions.](#45-openmp-for-multi-core-executions)

## 4.1 General Information

Detailed information about the CPU and GPU implementation can be found in the papers:

* Crespo AJC, Domínguez JM, Rogers BD, Gómez-Gesteira M, Longshaw S, Canelas R, Vacondio R, Barreiro A, García-Feal O. 2015. DualSPHysics: open-source parallel CFD solver on Smoothed Particle Hydrodynamics (SPH). Computer Physics Communications, 187: 204-216. doi: 10.1016/j.cpc.2014.10.004
* Domínguez JM, Crespo AJC, Valdez-Balderas D, Rogers BD. and Gómez-Gesteira M. 2013. New multi-GPU implementation for Smoothed Particle Hydrodynamics on heterogeneous clusters. Computer Physics Communications, 184: 1848-1860. doi: 10.1016/j.cpc.2013.03.008
* Domínguez JM, Crespo AJC and Gómez-Gesteira M. 2013. Optimization strategies for CPU and GPU implementations of a smoothed particle hydrodynamics method. Computer Physics Communications, 184(3): 617-627. doi:10.1016/j.cpc.2012.10.015
* Crespo AJC, Domínguez JM, Barreiro A, Gómez-Gesteira M and Rogers BD. 2011.  GPUs, a new tool of acceleration in CFD: Efficiency and reliability on Smoothed Particle Hydrodynamics methods. PLoS ONE, 6(6), e20685. doi:10.1371/journal.pone.0020685
* Domínguez JM, Crespo AJC, Gómez-Gesteira M, Marongiu, JC. 2011. Neighbour lists in Smoothed Particle Hydrodynamics. International Journal For Numerical Methods in Fluids, 67(12): 2026-2042. doi: 10.1002/fld.2481

## 4.2 Main Steps of the Code

The DualSPHysics code is the result of an optimised implementation using the best
approaches for CPU and GPU with the accuracy, robustness and reliability shown by
the SPHysics code. SPH simulations such as those in the SPHysics and DualSPHysics
codes can be split in three main steps; 
* generation of the neighbour list, 
* computation of the forces between particles (solving momentum and continuity equations) and 
* the update of the physical quantities at the next time step. 

Thus, running a simulation means executing these steps in an iterative manner:

1. First step: Neighbour list (Cell-linked list described in [Domínguez et al., 2011]):
* Domain is divided into square cells of side 2h (or the size of the kernel domain).
* A list of particles, ordered according to the cell to which they belong, is generated.
* All the arrays with the physical variables belonging the particles are reordered
according the list of particles.

2. Second step: Force computation:
* Particles of the same cell and adjacent cells are candidates to be neighbours.
* Each particle interacts with all its neighbouring particles (at a distance < 2h).

3. Third step: System Update:
* New time step is computed.
* Physical quantities for the next step are updated starting from the values of physical
variables at the present or previous time steps using the particle interactions.
* Particle information (velocity and density) are saved on local storage (the hard
drive) at defined times.


<p align="center">
<img src="https://i.imgur.com/HsNQmBF.png"/>
</p>

<p align="center">
<strong>Figure 4-1.</strong> Flow diagram of the CPU (left) and total GPU implementation (right).
</p>

## 4.3 GPU Implementation
The GPU implementation is focused on the force computation since following [Domínguez et al., 2011] this is the most consuming part in terms of runtime. However the most efficient technique consists of minimising the communications between the CPU and GPU for the data transfers. If neighbour list and system update are also implemented on the GPU the CPU-GPU memory transfer is needed at the beginning of the simulation while relevant data will be transferred to the CPU when saving output data is required (usually infrequently). [Crespo et al., 2011] used an execution of DualSPHysics performed entirely on the GPU to run a numerical experiment where the results are in close agreement with the experimental results.

The GPU implementation presents some key differences in comparison to the CPU version. The main difference is the parallel execution of all tasks that can be parallelised such as all loops regarding particles. One GPU execution thread computes the resulting force of one particle performing all the interactions with its neighbours. Different to previous versions, in version 4.0 onwards, the symmetry of the particle interaction is not employed on the CPU, the same as in the GPU implementation.  On a GPU it is not efficient due to memory coalescence issues. Now, the new CPU structure mimics the GPU threads, which ensures continuity of coding and structure (hence ease of debugging, etc.).

DualSPHysics is unique where the same application can be run using either the CPU or GPU implementation; this facilitates the use of the code not only on workstations with an Nvidia GPU but also on machines without a CUDA-enabled GPU. The CPU version is parallelised using the OpenMP API. The main code has a common core for both the CPU and GPU implementations with only minor source code differences implemented for the two devices applying the specific optimizations for CPU and GPU. Thus, debugging or maintenance is easier and comparisons of results and computational time are more direct. 

## 4.4 Double Precision
Most of the real variables in DualSPHysics are coded in double precision, but particle data uses single precision since double precision is usually not necessary and the use of single precision improves the performance (especially on GPU computing). However, the use of double precision is mandatory on position variables for high resolution simulations with large domains (Domínguez et al., 2014). The precision of particle position is critical in two operations: the calculation of interaction distance between particles in PI and the update of particle magnitudes for next simulation step in SU. The position variables are implemented in DualSPHysics using double precision and the SU task is performed by using double precision for both architectures (CPU and GPU). However, the relative position of the particle within its cell using single precision is computed before particle interaction and used to compute the distance between particles on GPU implementation. This optimisation provides a significant performance improvement without loss of accuracy on GPU models where performance ratio between double and single precision computing is 1/32. This optimisation is not applied to CPU code since the achieved speedup on CPU is negligible.

## 4.5 OpenMP for multi-core executions.
In the new version, the CPU implementation aims to achieve a higher performance in
the current machines that contains many cores. Now, the interactions of a given
particles with all its neighbours are carried out by the same execution thread. Symmetry
in the force computation is not applied in order to increase the parallelization level of
the algorithms. Previous versions of DualSPHysics were fast on CPUs with 4-8 cores
but efficiency decreases significantly with number of cores. Furthermore, memory
consumption increases with more cores. The new CPU code achieves an efficiency of
86.2% simulating 150,000 particles with 32 cores while the same execution achieved an
efficiency of only 59.7% with previous version 3.0 and without memory increase. The
current OpenMP implementation is not only fast and efficient with many cores, but also
offers more advantages for users that want to modify the code; implementation is now
easier since symmetry is removed during force computation such that the CPU code is
more similar to the GPU code which facilitates its comprehension and editing.



