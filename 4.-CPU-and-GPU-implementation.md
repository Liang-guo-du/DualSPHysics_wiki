### General Information

Detailed information about the CPU and GPU implementation can be found in the papers:

* Crespo AJC, Domínguez JM, Rogers BD, Gómez-Gesteira M, Longshaw S, Canelas R, Vacondio R, Barreiro A, García-Feal O. 2015. DualSPHysics: open-source parallel CFD solver on Smoothed Particle Hydrodynamics (SPH). Computer Physics Communications, 187: 204-216. doi: 10.1016/j.cpc.2014.10.004
* Domínguez JM, Crespo AJC, Valdez-Balderas D, Rogers BD. and Gómez-Gesteira M. 2013. New multi-GPU implementation for Smoothed Particle Hydrodynamics on heterogeneous clusters. Computer Physics Communications, 184: 1848-1860. doi: 10.1016/j.cpc.2013.03.008
* Domínguez JM, Crespo AJC and Gómez-Gesteira M. 2013. Optimization strategies for CPU and GPU implementations of a smoothed particle hydrodynamics method. Computer Physics Communications, 184(3): 617-627. doi:10.1016/j.cpc.2012.10.015
* Crespo AJC, Domínguez JM, Barreiro A, Gómez-Gesteira M and Rogers BD. 2011.  GPUs, a new tool of acceleration in CFD: Efficiency and reliability on Smoothed Particle Hydrodynamics methods. PLoS ONE, 6(6), e20685. doi:10.1371/journal.pone.0020685
* Domínguez JM, Crespo AJC, Gómez-Gesteira M, Marongiu, JC. 2011. Neighbour lists in Smoothed Particle Hydrodynamics. International Journal For Numerical Methods in Fluids, 67(12): 2026-2042. doi: 10.1002/fld.2481


The DualSPHysics code is the result of an optimised implementation using the best
approaches for CPU and GPU with the accuracy, robustness and reliability shown by
the SPHysics code. SPH simulations such as those in the SPHysics and DualSPHysics
codes can be split in three main steps; 
* generation of the neighbour list, 
* computation of the forces between particles (solving momentum and continuity equations) and 
* the update of the physical quantities at the next time step. 

Thus, running a simulation means executing these steps in an iterative manner:

1. First step: Neighbour list (Cell-linked list described in [Domínguez et al., 2011]):
* Domain is divided into square cells of side 2h (or the size of the kernel domain).
* A list of particles, ordered according to the cell to which they belong, is generated.
* All the arrays with the physical variables belonging the particles are reordered
according the list of particles.

2. Second step: Force computation:
* Particles of the same cell and adjacent cells are candidates to be neighbours.
* Each particle interacts with all its neighbouring particles (at a distance < 2h).

3. Third step: System Update:
* New time step is computed.
* Physical quantities for the next step are updated starting from the values of physical
variables at the present or previous time steps using the particle interactions.
* Particle information (velocity and density) are saved on local storage (the hard
drive) at defined times.

The GPU implementation is focused on the force computation since following [Domínguez et al., 2011] this is the most consuming part in terms of runtime. However the most efficient technique consists of minimising the communications between the CPU and GPU for the data transfers. If neighbour list and system update are also implemented on the GPU the CPU-GPU memory transfer is needed at the beginning of the simulation while relevant data will be transferred to the CPU when saving output data is required (usually infrequently). [Crespo et al., 2011] used an execution of DualSPHysics performed entirely on the GPU to run a numerical experiment where the results are in close agreement with the experimental results.

The GPU implementation presents some key differences in comparison to the CPU version. The main difference is the parallel execution of all tasks that can be parallelised such as all loops regarding particles. One GPU execution thread computes the resulting force of one particle performing all the interactions with its neighbours. Different to previous versions, in version 4.0 onwards, the symmetry of the particle interaction is not employed on the CPU, the same as in the GPU implementation.  On a GPU it is not efficient due to memory coalescence issues. Now, the new CPU structure mimics the GPU threads, which ensures continuity of coding and structure (hence ease of debugging, etc.).

DualSPHysics is unique where the same application can be run using either the CPU or GPU implementation; this facilitates the use of the code not only on workstations with an Nvidia GPU but also on machines without a CUDA-enabled GPU. The CPU version is parallelised using the OpenMP API. The main code has a common core for both the CPU and GPU implementations with only minor source code differences implemented for the two devices applying the specific optimizations for CPU and GPU. Thus, debugging or maintenance is easier and comparisons of results and computational time are more direct. 

<p align="center">
<img src="https://i.imgur.com/HsNQmBF.png"/>
</p>

<p align="center">
<strong>Figure 4-1.</strong> Flow diagram of the CPU (left) and total GPU implementation (right).
</p>

### Double Precision
The parallel computing power of Graphics Processing Units (GPUs) has led to an important increase in the size of the simulations but problems of precision can appear when simulating large domains with high resolution (especially in 2-D simulations). Hence, there are numerous simulations that require a small particle size “dp” relative to the computational domain [Domínguez et al., 2013c], namely fine resolution or long domains. Since DualSPHysics an implementation with double precision is included. For example, arrays of position now use double precision and updating state of particles is also implemented with double precision.

### OpenMP for multi-core executions.
In the new version, the CPU implementation aims to achieve a higher performance in
the current machines that contains many cores. Now, the interactions of a given
particles with all its neighbours are carried out by the same execution thread. Symmetry
in the force computation is not applied in order to increase the parallelization level of
the algorithms. Previous versions of DualSPHysics were fast on CPUs with 4-8 cores
but efficiency decreases significantly with number of cores. Furthermore, memory
consumption increases with more cores. The new CPU code achieves an efficiency of
86.2% simulating 150,000 particles with 32 cores while the same execution achieved an
efficiency of only 59.7% with previous version 3.0 and without memory increase. The
current OpenMP implementation is not only fast and efficient with many cores, but also
offers more advantages for users that want to modify the code; implementation is now
easier since symmetry is removed during force computation such that the CPU code is
more similar to the GPU code which facilitates its comprehension and editing.

### Optimisation of the size of blocks for execution of CUDA kernels.
The size of the blocks is a key parameter in the execution of CUDA kernels since it can lead to performance differences of 50%. This variation becomes more significant in the kernels that compute particle interactions since take more than 90% of the total execution time.

Since version 4.0 a new automatic estimation of the optimum block size of CUDA kernels (particle interactions on GPU) is included. This optimum block size depends on: (i) features of the kernel (registers and shared memory), (ii) compilation parameters and the CUDA version, (iii) hardware to be used and GPU specifications and (iv) input data to be processed by the kernel (divergence, memory coalescent access). The CUDA Occupancy Calculator is available from CUDA version 6.5.

<br>

It is worth to mention, that the DualSPHysics package includes not only the SPH solver (named DualSPHysics code) but also dedicated pre-processing software (that can use a whole range of different input files for the geometries including CAD, STL, PLY files, etc., making setting up simulations straightforward) and advanced post-processing tools (that allow users to measure physical magnitudes of any flow property at arbitrary locations in the domain).

The figure 4-2 includes the number of code lines of the different codes where:
* DualSPHysics is the SPH solver
* GenCase is the pre-processing tool
* All the others are the post-processing tools

<p align="center">
<img src="https://i.imgur.com/rI4rBtT.png"/>
</p>

<p align="center">
<strong>Figure 4-2.</strong> Number of code lines of DualSPHysics code and the pre- and post-processing tools.
</p>